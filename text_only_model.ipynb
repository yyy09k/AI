{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c314e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import RobertaTokenizer, RobertaModel  # 使用 RoBERTa\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # 学习率调度器\n",
    "from torch.utils.tensorboard import SummaryWriter  # 导入 TensorBoard 相关的库\n",
    "from tqdm import tqdm  # 用于显示训练进度条\n",
    "\n",
    "# 配置设备\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# 自定义 collate_fn 处理变长输入\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    该函数用于将单个数据样本从 Dataset 中合并成一个批次。\n",
    "    主要功能：\n",
    "    1. 合并文本输入，处理每个文本的 `input_ids` 和 `attention_mask`，并做 padding 以保证输入长度一致。\n",
    "    2. 合并标签，将标签转换为张量。\n",
    "    '''\n",
    "    text_inputs, labels = zip(*batch)\n",
    "\n",
    "    # 获取所有文本输入的 input_ids 和 attention_mask\n",
    "    input_ids = [text_input['input_ids'].squeeze(0) for text_input in text_inputs]\n",
    "    attention_mask = [text_input['attention_mask'].squeeze(0) for text_input in text_inputs]\n",
    "\n",
    "    max_length = max([input_id.size(0) for input_id in input_ids])\n",
    "\n",
    "    # 对文本输入做 padding，确保所有文本输入的长度一致\n",
    "    for i in range(len(input_ids)):\n",
    "        padding_length = max_length - input_ids[i].size(0)\n",
    "        if padding_length > 0:\n",
    "            input_ids[i] = torch.cat([input_ids[i], torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "            attention_mask[i] = torch.cat([attention_mask[i], torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    attention_mask = torch.stack(attention_mask, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask}, labels\n",
    "\n",
    "# 仅文本数据的加载类\n",
    "class TextOnlyDataset(Dataset):\n",
    "    def __init__(self, data_folder, label_file, tokenizer, max_length=512):\n",
    "        '''\n",
    "        该类用于加载和处理仅包含文本的数据。\n",
    "        '''\n",
    "        self.data_folder = data_folder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.data = pd.read_csv(label_file)\n",
    "        self.guid_list = self.data[\"guid\"].tolist()\n",
    "        self.labels = self.data[\"tag\"].map({\"positive\": 0, \"neutral\": 1, \"negative\": 2}).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.guid_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        获取指定索引的数据样本（文本和标签）\n",
    "        '''\n",
    "        guid = self.guid_list[idx]\n",
    "        text_path = os.path.join(self.data_folder, f\"{guid}.txt\")\n",
    "\n",
    "        with open(text_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        text_input = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=self.max_length)\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return text_input, label\n",
    "\n",
    "# 仅文本模型（RoBERTa）\n",
    "class TextOnlyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        定义仅文本模型，使用 RoBERTa 进行文本特征提取。\n",
    "        '''\n",
    "        super(TextOnlyModel, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "        self.fc = nn.Linear(self.roberta.config.hidden_size, 3)  # 输出3个类别：positive, neutral, negative\n",
    "\n",
    "    def forward(self, text_input):\n",
    "        '''\n",
    "        前向传播，提取文本特征并进行分类。\n",
    "        '''\n",
    "        text_output = self.roberta(**text_input)\n",
    "        output = self.fc(text_output.pooler_output)\n",
    "        return output\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=5, learning_rate=1e-5, save_path=\"model.pth\"):\n",
    "    '''\n",
    "    该函数用于训练仅文本模型。\n",
    "    1. 定义优化器和损失函数（交叉熵损失）。\n",
    "    2. 每个 epoch 中计算训练损失和准确率。\n",
    "    3. 评估模型在验证集上的表现。\n",
    "    4. 保存训练过程中最好的模型。\n",
    "    '''\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.1, verbose=True)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    writer = SummaryWriter(log_dir='./runs/text_only_model')  # 用于 TensorBoard 记录\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # 使用 tqdm 显示训练进度条\n",
    "        for data, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data = {key: val.squeeze(1).to(device) for key, val in data.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "\n",
    "        train_losses.append(total_loss / len(train_loader))\n",
    "        train_accuracy = correct_train / total_train * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # 验证集评估\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}/{epochs}\", unit=\"batch\"):\n",
    "                data = {key: val.squeeze(1).to(device) for key, val in data.items()}\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(data)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "        val_accuracy = correct_val / total_val * 100\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        # 在 TensorBoard 中记录损失与准确率\n",
    "        writer.add_scalar('Training Loss', train_losses[-1], epoch)\n",
    "        writer.add_scalar('Validation Loss', val_losses[-1], epoch)\n",
    "        writer.add_scalar('Training Accuracy', train_accuracies[-1], epoch)\n",
    "        writer.add_scalar('Validation Accuracy', val_accuracies[-1], epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_losses[-1]}, Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "        print(f\"Validation Loss: {val_losses[-1]}, Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "\n",
    "        # 更新学习率\n",
    "        scheduler.step(val_losses[-1])\n",
    "\n",
    "    # 绘制训练过程的损失和准确率曲线\n",
    "    plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "    # 关闭 TensorBoard 记录器\n",
    "    writer.close()\n",
    "\n",
    "# 设置分词器\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# 加载仅文本数据集\n",
    "train_dataset = TextOnlyDataset(data_folder='./data', label_file='./train.txt', tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 初始化模型\n",
    "model = TextOnlyModel().to(device)\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, val_loader, epochs=5, learning_rate=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
